<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>On Guessing Well</title>
    <link>/</link>
    <description>Recent content on On Guessing Well</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 23 Aug 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Making Good Decisions</title>
      <link>/post/making-good-decisions/</link>
      <pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/making-good-decisions/</guid>
      <description>Bottom Line Up Front This post introduces the basics of decision making under uncertainty. It was written for anyone interested in learning how to use decision theory to solve real-world problems.
 Learning Objectives In this post you will learn about:
 How to identify decision problems in the real-world. How to frame decision problems in terms of actions, outcomes, utility, and probability. How to solve decision problems by maximizing expected utility.</description>
    </item>
    
    <item>
      <title>This OR That</title>
      <link>/post/this-or-that/</link>
      <pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/this-or-that/</guid>
      <description>Today I wanted to write a short post on the sum rule of probability. Specifically I wanted to show how to derive the sum rule from the three axioms of probability.
Recall from previous posts this statement of Kolmogorov’s axioms:
\(P(\omega)\) is the probability that an event \(\omega\) occurs if \(P\) satisfies the following:
\(P(\omega) \in \mathbb{R_{\ge0}}\) i.e. the probability is a non-negative real number.
 \(P(\Omega) = 1\) where \(\Omega\) is the set of all possible outcomes.</description>
    </item>
    
    <item>
      <title>Setting Expectations</title>
      <link>/post/setting-expectations/</link>
      <pubDate>Sat, 04 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/setting-expectations/</guid>
      <description>This one is a short(ish) one (just so I can keep up the momentum of posting). I absolutely love the concept of mathematical expectations, and in this post I am hoping to introduce expectations and share one of my favorite factoids about them.
First things first, let’s remind ourselves of what a probability is. Over simplifying (my apologies to the measure theorists out there), we say that \(P(\omega)\) is the probability that an event \(\omega\) occurs if \(P\) satisfies the following axioms (courtesy of Kolmogorov):</description>
    </item>
    
    <item>
      <title>Marginal Likelihoods of Models Defined on Binary Sequences</title>
      <link>/post/marginal-likelihoods-of-models-defined-on-binary-sequences/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/marginal-likelihoods-of-models-defined-on-binary-sequences/</guid>
      <description>This post is a long time coming. I originally wrote it in January of 2020 right when my partner and I moved from Washington, DC to Seattle, WA. Needless to say, the COVID-19 pandemic dramatically shifted how we were to live our life, and since both of us work in emergency management my free time (or at least my free energy) to work on this blog became non-existent. That being said, I am glad to have the opportunity to revisit this post, possibly with a little more wisdom (probably not).</description>
    </item>
    
    <item>
      <title>The Relationship Between Resourcing and Performance in Workload Modeling</title>
      <link>/post/the-relationship-between-resourcing-and-performance-in-workload-modeling/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-relationship-between-resourcing-and-performance-in-workload-modeling/</guid>
      <description>\[r = \frac{w}{sc}\]
\[s_0 = \frac{w}{r_0c}\]
\[ \begin{aligned} \frac{s_0}{s} &amp;amp;= \frac{\frac{w}{r_0c}}{\frac{w}{rc}} \\ \\ &amp;amp;= \frac{w}{r_0c}{\frac{rc}{w}} \\ \\ &amp;amp;= \frac{r}{r_0} \\ \end{aligned} \]
\[ E\Big(\frac{S_0}{s}\Big) = E\Big(\frac{R}{r_0}\Big) = \frac{1}{r_0}E(R)\]
\[P\Big(\frac{R}{r_0} = \frac{r}{r_0}\Big) = P(R = r)\]
library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.1 ✓ dplyr 1.0.0 ## ✓ tidyr 1.1.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.</description>
    </item>
    
    <item>
      <title>The Sleeping Beauty Problem</title>
      <link>/post/the-sleeping-beauty-problem/</link>
      <pubDate>Sun, 17 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-sleeping-beauty-problem/</guid>
      <description>I just learned about the “Sleeping Beauty” problem in a wonderful philosophy of probability anthology edited by Anthony Eagle. The problem is raised in chapter 10 via an essay by Adam Elga. I will quote the passage directly:
 “Some researches are going to put you to sleep. During the two days that your sleep lasts, they will briefly wake you up either once or twice depending on the toss of a fair coin (Heads: once; Tails: twice).</description>
    </item>
    
    <item>
      <title>Rebooting the Blog</title>
      <link>/post/rebooting-the-blog/</link>
      <pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/rebooting-the-blog/</guid>
      <description>Greetings and salutations wonderful humans,
For those who have been following the blog, you may have noticed that my previous posts are no longer available. This is because I am rebooting the blog. I felt that the posts up until now strayed too far away from the central theme of the blog, and felt far less polished than I would like. Additionally, I focused too much on mathematical notation and theory, and not enough on inutition and application (let alone featuring more code!</description>
    </item>
    
    <item>
      <title>Bernoulli and Binomial Distribution</title>
      <link>/post/bernoulli-and-binomial-distribution/</link>
      <pubDate>Mon, 19 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bernoulli-and-binomial-distribution/</guid>
      <description>Today’s post will explore a fundamental probability distribution and its generalization: the Bernoulli and binomial distribution. The Bernoulli distribution is named after Jacob Bernoulli, one of several famous mathematicians from the Bernoulli family. We can use this distribution to represent the probability of a single experiment with a binary (bīnārius is Latin for “consisting of two”) outcome. Classically, this probability setup can be thought of as the outcome of a single coin toss, however it can be generalized to a great many practical applications.</description>
    </item>
    
    <item>
      <title>A Closer Look at Inductive Generalization and the Statistical Syllogism</title>
      <link>/post/a-closer-look-at-inductive-generalization-and-the-statistical-syllogism/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/a-closer-look-at-inductive-generalization-and-the-statistical-syllogism/</guid>
      <description>A recent post compared and contrasted deduction, induction, and abduction. Within that post we touched on inductive generalization (inferring characteristics of population from a sample), and the statistical syllogism (inferring characteristics of a sample from a population). I briefly wanted to spend a little more time with each of these concepts, and talk about what conclusions are plausible (i.e. not necessarily true or false given what we know) when applying them.</description>
    </item>
    
    <item>
      <title>A Closer Look at the Statistical Syllogism and Inductive Generalization</title>
      <link>/post/a-closer-look-at-the-statistical-syllogism-and-inductive-generalization/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/a-closer-look-at-the-statistical-syllogism-and-inductive-generalization/</guid>
      <description>A recent post compared and contrasted deduction, induction, and abduction. Within that post we touched on both the statistical syllogism (inferring characteristics of a sample from a population), and inductive generalization (inferring characteristics of population from a sample). I wanted to spend a little more time with each of these concepts to get a better sense of their justification and how we can interpret them. My hope is that through this post, we will gain some intuition of probable reasoning; and experience the interplay between induction, abduction, and deduction.</description>
    </item>
    
    <item>
      <title>Chinese Resaurants and Indian Buffets</title>
      <link>/post/chinese-restaurants-and-indian-buffets/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/chinese-restaurants-and-indian-buffets/</guid>
      <description>Today I wanted to talk about one of my all time favorite subjects in probability theory: stochastic processes. Many stochastic processes can be thought of as sequences of random variables. For instance a Bernoulli process is a sequence of binary random variables, which can be though of as a listing of the outcomes of successive coin flips. Today we are going to talk about an interesting set of stochastic processes that are closely related to one of the most fundamental idea in probability and statistics: sampling.</description>
    </item>
    
    <item>
      <title>The Principle of Insufficient Reason</title>
      <link>/post/the-principle-of-insufficient-reason/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-principle-of-insufficient-reason/</guid>
      <description>  Definition History Example Rule of Succession Apparent Paradox  Coin with three sides Continious Distributions  Generalization via the Principle of Maximum Entropy  </description>
    </item>
    
    <item>
      <title>The Principle of Maximum Entropy</title>
      <link>/post/the-principle-of-maximum-entropy/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-principle-of-maximum-entropy/</guid>
      <description>  Information Content/Surprisal Entropy Maximizing Entropy Given Constraints Lagrangian Multipliers Exponential Distributions Applications Likelihoods Priors Example  </description>
    </item>
    
    <item>
      <title>An Introduction to Deductive Reasoning</title>
      <link>/post/an-introduction-to-deductive-reasoning/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/an-introduction-to-deductive-reasoning/</guid>
      <description>One of my absolute favorites books is Probability: the Logic of Science by E.T. Jaynes. The book was published posthumously in 2003, after Jayne’s death in 1998. While the book presents a compelling philosophical argument for probability theory, it is known to be incomplete and to contain errors. Despite this, I think it is a very important work in the history of probability theory, and a great foundation for how one can reason with incomplete information.</description>
    </item>
    
    <item>
      <title>Reasoning via Deduction, Induction, and Abduction</title>
      <link>/post/reasoning-via-deduction-induction-and-abduction/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/reasoning-via-deduction-induction-and-abduction/</guid>
      <description>My last post was an introduction to deductive reasoning, that is reasoning via deductive argument. I wanted to briefly outline two other forms of reasoning so we can identify their usage and compare their strengths and weakness in future applications.
Deduction As we stated in our last post, deduction is reasoning that, given premises, guarantees the truth of its conclusion. Two major forms of deductive argument are “Modus Ponens” and “Modus Tollens”.</description>
    </item>
    
    <item>
      <title>Probability in a Finite and Deterministic Universe</title>
      <link>/post/probability-in-a-finite-and-deterministic-universe/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/probability-in-a-finite-and-deterministic-universe/</guid>
      <description>My previous post got me thinking about the nature of infinity. It lead me to a thought provoking post by Steve Patterson on logical errors regarding infinite sets. While I haven’t taken enough time to fully appreciate his logic, it got me thinking about how probability relates to metaphysics and epistemology. Steve’s claim is that modern mathematics has made a mistake with regards to infinity, specifically that by definition a set cannot be infinite (or else it would be finite), and thus arguments built on this premise are unsound (though the logic, granting the premise may be valid).</description>
    </item>
    
    <item>
      <title>Some Oddities of the Principle of Insufficient Reason</title>
      <link>/post/some_oddities_of_the-principle-of-insufficient-reason/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/some_oddities_of_the-principle-of-insufficient-reason/</guid>
      <description>We are keenly interested in reasoning with incomplete or uncertain information i.e. how we should make a “best guess” given that we have imperfect knowledge. One of the classical tools in our tool belt is the so called “principle of insufficient reason” also known as the “principle of indifference”. More on the history and extensions of the principle, but for now I want to focus on some oddities that occur by trying to reason with the principle.</description>
    </item>
    
    <item>
      <title>An Introduction to Workload Modeling</title>
      <link>/post/an-introduction-to-workload-modeling/</link>
      <pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/an-introduction-to-workload-modeling/</guid>
      <description>The Purpose of this Post This post presents a brief introduction to workload modeling. For our purposes we define workload modeling as “an effort to usefully represent the relationship between A) the demand for products (or services), B) the production (or service delivery) process, C) and the requirement for resources”. This should be taken as a working definition, as we are sure that our own investigation into this subject will shape and change our fundamental understanding in the future.</description>
    </item>
    
    <item>
      <title>A First Attempt w/ Central Tendency</title>
      <link>/post/a-first_attempt-with-central-tendency/</link>
      <pubDate>Mon, 18 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/a-first_attempt-with-central-tendency/</guid>
      <description>The purpose of this blog is to explore how we can reason about what we know to make guesses about what we don’t. We are going to “dip our toes” into this subject by exploring one of the most commonly experienced concepts in probability theory and statistics: measures of central tendency. Specifically, we are going to revisit the common definitions of measures such as the mean, median, and mode; and attempt to build a stronger intuition of what they are and why they are useful in our goal of making better guesses.</description>
    </item>
    
    <item>
      <title>About This Blog</title>
      <link>/about/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>Greetings wonderful humans,
My name is David W.C. Telson and this blog is all about decision making under uncertainty. We will be exploring all sorts of topics from the realms of probability theory, decision theory, statistics, machine learning, and the philosophy of science.
At present I am a Data Scientist working for the Federal Emergency Management Agency in Seattle, WA (formerly Washington, DC). Most of the problems I work on involve leveraging machine learning and statistics to support decisions about the Agency&amp;rsquo;s incident workforce and field operations.</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/2020-08-01-making-a-good-guess/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/post/2020-08-01-making-a-good-guess/</guid>
      <description>Make Good Choices /*! jQuery v1.11.3 | (c) 2005, 2015 jQuery Foundation, Inc. | jquery.org/license */ !function(a,b){&#34;object&#34;==typeof module&amp;&amp;&#34;object&#34;==typeof module.exports?module.exports=a.document?b(a,!0):function(a){if(!a.document)throw new Error(&#34;jQuery requires a window with a document&#34;);return b(a)}:b(a)}(&#34;undefined&#34;!=typeof window?window:this,function(a,b){var c=[],d=c.slice,e=c.concat,f=c.push,g=c.indexOf,h={},i=h.toString,j=h.hasOwnProperty,k={},l=&#34;1.11.3&#34;,m=function(a,b){return new m.fn.init(a,b)},n=/^[\s\uFEFF\xA0]+|[\s\uFEFF\xA0]+$/g,o=/^-ms-/,p=/-([\da-z])/gi,q=function(a,b){return b.toUpperCase()};m.fn=m.prototype={jquery:l,constructor:m,selector:&#34;&#34;,length:0,toArray:function(){return d.call(this)},get:function(a){return null!=a?0a?this[a+this.length]:this[a]:d.call(this)},pushStack:function(a){var b=m.merge(this.constructor(),a);return b.prevObject=this,b.context=this.context,b},each:function(a,b){return m.each(this,a,b)},map:function(a){return this.pushStack(m.map(this,function(b,c){return a.call(b,c,b)}))},slice:function(){return this.pushStack(d.apply(this,arguments))},first:function(){return this.eq(0)},last:function(){return this.eq(-1)},eq:function(a){var b=this.length,c=+a+(0a?b:0);return this.pushStack(c=0&amp;&amp;bc?[this[c]]:[])},end:function(){return this.prevObject||this.constructor(null)},push:f,sort:c.sort,splice:c.splice},m.extend=m.fn.extend=function(){var a,b,c,d,e,f,g=arguments[0]||{},h=1,i=arguments.length,j=!1;for(&#34;boolean&#34;==typeof g&amp;&amp;(j=g,g=arguments[h]||{},h++),&#34;object&#34;==typeof g||m.isFunction(g)||(g={}),h===i&amp;&amp;(g=this,h--);ih;h++)if(null!=(e=arguments[h]))for(d in e)a=g[d],c=e[d],g!==c&amp;&amp;(j&amp;&amp;c&amp;&amp;(m.isPlainObject(c)||(b=m.isArray(c)))?(b?(b=!1,f=a&amp;&amp;m.isArray(a)?a:[]):f=a&amp;&amp;m.isPlainObject(a)?a:{},g[d]=m.extend(j,f,c)):void 0!==c&amp;&amp;(g[d]=c));return g},m.extend({expando:&#34;jQuery&#34;+(l+Math.random()).replace(/\D/g,&#34;&#34;),isReady:!0,error:function(a){throw new Error(a)},noop:function(){},isFunction:function(a){return&#34;function&#34;===m.type(a)},isArray:Array.isArray||function(a){return&#34;array&#34;===m.type(a)},isWindow:function(a){return null!=a&amp;&amp;a==a.window},isNumeric:function(a){return!m.isArray(a)&amp;&amp;a-parseFloat(a)+1=0},isEmptyObject:function(a){var b;for(b in a)return!1;return!0},isPlainObject:function(a){var b;if(!a||&#34;object&#34;!==m.type(a)||a.nodeType||m.isWindow(a))return!1;try{if(a.constructor&amp;&amp;!j.call(a,&#34;constructor&#34;)&amp;&amp;!j.call(a.constructor.prototype,&#34;isPrototypeOf&#34;))return!1}catch(c){return!1}if(k.ownLast)for(b in a)return j.call(a,b);for(b in a);return void 0===b||j.call(a,b)},type:function(a){return null==a?a+&#34;&#34;:&#34;object&#34;==typeof a||&#34;</description>
    </item>
    
    <item>
      <title>The Guessing Game</title>
      <link>/post/the-guessing-game/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/post/the-guessing-game/</guid>
      <description>Greetings wonderful humans,
Today’s post is about what I call the “guessing game”, or alternatively the “prediction problem”. The guessing game asks a simple question: how do we make a best guess as to the unknown value of some object of interest? This question is at the core of this blog; and I would argue that it lies at the heart of probability theory, statistics, machine learning, decision theory, and science itself!</description>
    </item>
    
  </channel>
</rss>