<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="referrer" content="no-referrer">
  

  <link rel="icon" type="image/png" href="/favicon.png">

  <title>
    
    
     Bernoulli and Binomial Distribution 
    
  </title>
  <link rel="canonical" href="/post/bernoulli-and-binomial-distribution/">

  <link rel="stylesheet" href="/css/fonts.css" />
  <link rel="stylesheet" href="/css/style.css" />

  
</head>

<body>
<section id=nav>
  <h1><a href="/">On Guessing Well</a></h1>
  <ul>
    
    <li><a href="/about">About</a></li>
    
    <li><a href="/">Posts</a></li>
    
    <li><a href="https://github.com/wctelson">GitHub</a></li>
    
    <li><a href="https://www.linkedin.com/in/davidwcnelson/">LinkedIn</a></li>
    
    <li><a href="https://twitter.com/wctelson">Twitter</a></li>
    
    <li><a href="/index.xml">RSS</a></li>
    
  </ul>
</section>


<section id=content>
  <h1> Bernoulli and Binomial Distribution </h1>

  <div id=sub-header>
    David W.C. Telson · 2019/08/19 · 4 minute read
  </div>

  <div class="entry-content">
    


<p>Today’s post will explore a fundamental probability distribution and its generalization: the Bernoulli and binomial distribution. The Bernoulli distribution is named after Jacob Bernoulli, one of several famous mathematicians from the Bernoulli family. We can use this distribution to represent the probability of a single experiment with a binary (bīnārius is latin for “consisting of two”) outcome. Classically, this probability setup can be thought of as the outcome of a single coin toss, however it can be generalized to a great many practical applications.</p>
<p>To get a bit more formal, let’s introduce some suitable notation. Let’s call <span class="math inline">\(Y\)</span> the unknown outcome of interest, which can take on one of the values in the set <span class="math inline">\(\{0,1\}\)</span>. We wish to define a probability mass faction (pmf) such that given one of the possible values for <span class="math inline">\(Y\)</span>, it will return the probability that <span class="math inline">\(Y\)</span> will take on that value.</p>
<p>Let’s suppose that <span class="math inline">\(P(Y = 1) = \theta\)</span> (the greek letter theta), that is the probability that <span class="math inline">\(Y\)</span> is equal to the outcome <span class="math inline">\(1\)</span> is some number <span class="math inline">\(\theta \in [0,1]\)</span>. Thanks to <a href="https://en.wikipedia.org/wiki/Probability_axioms">the axiom of unity</a>, we know that since <span class="math inline">\(P(Y = 1 \cup Y = 0) = 1 = P(Y = 1) + P(Y = 0)\)</span> we must have <span class="math inline">\(P(Y = 0) = 1 - \theta\)</span>. With this in place, we now have enough information to define a probability mass function. Let’s make a first attempt:</p>
<p><span class="math display">\[ P(Y = y) = \text{pmf}_Y(y)= 
\begin{cases}
    \theta,&amp; \text{if } y = 1\\
    1-\theta,              &amp; \text{if } y = 0\\
\end{cases}\]</span></p>
<p>This function as defined will return <span class="math inline">\(\theta\)</span> when provided <span class="math inline">\(1\)</span> and <span class="math inline">\(1 - \theta\)</span> when provided <span class="math inline">\(0\)</span> literally by definition. While technically correct, the conditional definition doesn’t look very streamlined, and as we shall see it limits us from discovering more general forms of our pmf. We can avoid the clunkiness of the original formula by replacing it with the following functional form:</p>
<p><span class="math display">\[P(Y = y) = \text{pmf}_Y(y) = \theta^y(1-\theta)^{1-y}\]</span></p>
<p>We can test the possible inputs of this function to see if it gives us the right probabilities. If <span class="math inline">\(y = 1\)</span>, then <span class="math inline">\(\theta^y(1-\theta)^{1-y} = \theta^1(1-\theta)^{1-1} = \theta\)</span>. If <span class="math inline">\(y = 0\)</span>, then <span class="math inline">\(\theta^y(1-\theta)^{1-y} = \theta^0(1-\theta)^{1-0} = (1-\theta)\)</span>. Indeed, our function works!</p>
<p>The Bernoulli is arguably the most fundamental probability model available. While there are many great applications of the model as is, it also serves as the foundation for a great many additional models that we will explore throughout this blog. Let’s explore the first of many generalizations: the binomial distribution.</p>
<p>While the Bernoulli distribution models the probability of an outcome of a single binary experiment (we use the term experiment loosely), the binomial (bi meaning two, and nomial meaning name) distribution extends the Bernoulli to multiple independent binary experiments by reimagining <span class="math inline">\(Y\)</span> not as an indicator for “success” or “failure”, but rather as the total number of successes observed after <span class="math inline">\(n\)</span> trails.</p>
<p>Let’s attempt to build such a pmf. We will begin will a brief review of “independence” in probability. Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are said to be independent (sometimes denoted <span class="math inline">\(A \perp B\)</span>) if <span class="math inline">\(P(A\cap B) = P(A)P(B)\)</span>. Thus a binomial distribution with <span class="math inline">\(n = 2\)</span> should be the product of two Bernoulli distributions. Let’s call the result of the first trial <span class="math inline">\(Y_1\)</span> and the result of the second <span class="math inline">\(Y_2\)</span>. Then a pmf might be:</p>
<p><span class="math display">\[P(Y_1 = y_1, Y_2 = y_2) = \text{pmf}_{Y_1, Y_2}(y_1, y_2) = \theta^{y_1}(1-\theta)^{1-{y_1}}\cdot\theta^{y_2}(1-\theta)^{1-{y_2}}\]</span></p>
<p>While this pmf is usable, it does have some rather unappealing properties. First, in order to specify <span class="math inline">\(n + 1\)</span> trials, we have to specify an additional outcome and add an additional term to our product. Second, if we just care about the number of success, we are unnecessarily specifying the order of our outcomes i.e. order doesn’t matter. This last insight is key to addressing both our concerns in deriving a new probability mass function.</p>
<p>For students of combinatorics, you may recall the “n choose k” function (notated as <span class="math inline">\(n \choose k\)</span>) that tells you how many combinations without repeition are possible when choosing <span class="math inline">\(k\)</span> items from a set of <span class="math inline">\(n\)</span> items. We can use this choose function to eliminated the redundancy our prior attempt at a binomial pmf created. Further, <span class="math inline">\(n \choose k\)</span> is referred to as a <a href="https://en.wikipedia.org/wiki/Binomial_coefficient">“binomial coefficient”</a>, hence the name of our distribution. Using this tool, we can write a new pmf:</p>
<p><span class="math display">\[P(Y = y) = \text{pmf}_Y(y) =  {n \choose y}\theta^y(1-\theta)^{n-y}\]</span></p>
<p>This new form is the canonical binomial distribution. You can see its relationship to the Bernoulli on the right hand side of the last identity. Keen readers will note that we can recover the Bernoulli by setting <span class="math inline">\(n = 1\)</span>.</p>
<p>With that, we close out today’s post. In the near future we will take this binomial distribution to its limit to derive a new pmf named the Poisson distribution.</p>
<p>Until next time!</p>

  </div>

  <div id=links>
    
      <a class="basic-alignment left" href="/post/a-closer-look-at-inductive-generalization-and-the-statistical-syllogism/">&laquo; A Closer Look at Inductive Generalization and the Statistical Syllogism</a>
    
    
      <a class="basic-alignment left" href="/post/the-prediction-problem/">The Prediction Problem &raquo;</a>
    
  </div>
</section>

<section id="comments">
<div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
      
      
      if (window.location.hostname == "localhost")
                return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = '';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
  
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>



</body>
</html>

