<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="referrer" content="no-referrer">
  

  <link rel="icon" type="image/png" href="/favicon.png">

  <title>
    
    
     A First Attempt w/ Central Tendency 
    
  </title>
  <link rel="canonical" href="/post/a-first_attempt-with-central-tendency/">

  <link rel="stylesheet" href="/css/fonts.css" />
  <link rel="stylesheet" href="/css/style.css" />

  
</head>

<body>
<section id=nav>
  <h1><a href="/">On Guessing Well</a></h1>
  <ul>
    
    <li><a href="/about">About</a></li>
    
    <li><a href="/">Posts</a></li>
    
    <li><a href="https://github.com/wctelson">GitHub</a></li>
    
    <li><a href="https://www.linkedin.com/in/davidwcnelson/">LinkedIn</a></li>
    
    <li><a href="https://twitter.com/wctelson">Twitter</a></li>
    
    <li><a href="/index.xml">RSS</a></li>
    
  </ul>
</section>


<section id=content>
  <h1> A First Attempt w/ Central Tendency </h1>

  <div id=sub-header>
    David W.C. Telson · 2019/02/18 · 18 minute read
  </div>

  <div class="entry-content">
    


<p>The purpose of this blog is to explore how we can reason about what we know to make guesses about what we don’t. We are going to “dip our toes” into this subject by exploring one of the most commonly experienced concepts in probability theory and statistics: measures of central tendency. Specifically, we are going to revisit the common definitions of measures such as the mean, median, and mode; and attempt to build a stronger intuition of what they are and why they are useful in our goal of making better guesses.</p>
<div id="primer-on-measures-of-central-tendency" class="section level1">
<h1>Primer on Measures of Central Tendency</h1>
<p>We begin with a quick primer on measures of central tendency. A measure of central tendency can be thought as some value that represents a “typical value” of a set of data. The three most commonly encountered are the Mean, Median and Mode:</p>
<div id="the-mean" class="section level2">
<h2>The Mean</h2>
<p>In the case of a sample (i.e. a collection of observations), the mean is the sum of the values of the observations divided by the total number of observations. The sample mean can be computed on interval and ratio data types. More generally, we say that the mean (aka the expected value) of a random variable is equal to the the sum of the possible outcomes weighted by each outcome’s probability of occurrence.</p>
<p>In symbols, the sample mean can be computed as:</p>
<p><span class="math display">\[Mean(X)= \frac{X_1 + X_2 + \dots + X_n}{n} = \frac{1}{n}\sum^n_{i=1}X_i\]</span> Where <span class="math inline">\(X_i\)</span> is the <span class="math inline">\(i\)</span>th observation of the <span class="math inline">\(n\)</span> total observations.</p>
</div>
<div id="the-median" class="section level2">
<h2>The Median</h2>
<p>In the case of a sample, the median is the value of the middle most observation, when observations are ordered by their values. When there are two middle values (as is the case with an even sample size), we often define the median as the mean of the two middle values. The sample median can be computed on ordinal, interval, and ratio data. More generally the median of a random variable is the quantity where the probability that the random variable will be less than or equal to it is 50%. This makes the median equivalent to the 50th percentile.</p>
<p>In symbols, the sample median can be computed as:</p>
<p><span class="math display">\[\begin{equation*}
    Median(X) = \begin{cases}
               X_{\big\lceil{\frac{n}{2}}\big\rceil}               &amp; if \space n \space is \space odd\\
               \frac{\bigg(X_{\frac{n}{2}} + X_{\frac{n}{2}+1}\bigg)}{2}               &amp;  if \space n \space is \space even\\
           \end{cases}
\end{equation*}\]</span></p>
<p>Where <span class="math inline">\(X\)</span> is a sample of observations of size <span class="math inline">\(n\)</span> and <span class="math inline">\(X_i\)</span> is the <span class="math inline">\(i\)</span>th observation.</p>
</div>
<div id="the-mode" class="section level2">
<h2>The Mode</h2>
<p>In the case of a sample, the mode is the value that occurs most frequently among all observations. The sample mode can be computed on nominal, ordinal, interval, and ratio data. More generally the mode of a random variable is the quantity that has the highest probability of occurrence. There may be more than one mode, possibly even as many modes as there are possibilities (in the case of equally likely outcomes).</p>
<p>In symbols, the sample mode can be computed as:</p>
<p><span class="math display">\[Mode(X) = \underset{x}{\operatorname{arg max}} \sum^n_{i=1}1_{\{X_i =x\}}  \]</span> Where <span class="math inline">\(x\)</span> is one of the values observed in the sample <span class="math inline">\(X\)</span> of size <span class="math inline">\(n\)</span>; <span class="math inline">\(X_i\)</span> is the <span class="math inline">\(i\)</span>th sample of <span class="math inline">\(X\)</span>; and <span class="math inline">\(1_{\{X_i =x\}}\)</span> is an indicator function, which is equal to <span class="math inline">\(1\)</span> if <span class="math inline">\(X_i = x\)</span> and <span class="math inline">\(0\)</span> otherwise. The <span class="math inline">\(\operatorname{arg max}\)</span> function returns the <span class="math inline">\(x\)</span> which maximizes the sum i.e. the value <span class="math inline">\(x\)</span> which occurs most frequently.</p>
</div>
</div>
<div id="setting-up-a-guessing-game" class="section level1">
<h1>Setting up a Guessing Game</h1>
<p>Now that we have briefly reviewed the mean, median, and mode, we want to build a better intuition for how the will assist us in our quest to make better guesses. In order to do this, we will setup a guessing game that we will seek to win.</p>
<p>The game is simple. There are 100 balls placed in a bin. Each ball has a number printed on it, with some numbers appearing on more balls than others. A ball will be drawn “at random” from the bin, meaning that each ball is equally likely to be drawn. Note that while each ball is equally likely, this does not mean the printed numbers are equally likely to be observed. Our goal is to guess at which ball will be drawn.</p>
<p>Let’s make this setup more concrete by programming it in R.</p>
<pre class="r"><code># set a seed to reproduce &quot;random&quot; results
set.seed(1)

# sampling 100 observations from a geometric distribution with probability parameter = .5
balls &lt;- rgeom(n = 100, prob = .25)

print(balls)</code></pre>
<pre><code>##   [1]  2  1  9  0  4  1  1  5  5  0  2 11  4 10  2  0  0  3  0  7  0  7  2
##  [24]  4  2  0  0  3  1  0  4  7  3  2  3  3  0  7  0  4  2  3  1  0  3  2
##  [47]  4  1  3  2  1  1  0  0  3  1  1  2  5  4  3  1  4  4  4  3  1  1  0
##  [70]  7  3  0  1  0  1  1  3  0  3  4 10  1  0  4  0  0  9  0  0  2  0  5
##  [93]  4  6  4  1  0  6  0  2</code></pre>
<p>Let’s compute our three measures of central tendency:</p>
<pre class="r"><code>require(tidyverse)

mean_printed_number &lt;- mean(balls)

median_printed_number &lt;- median(balls)

mode_printed_number &lt;- 
  balls %&gt;% 
  table %&gt;% 
  sort(decreasing = TRUE) %&gt;% 
  head(1) %&gt;% 
  names()

cat(&#39;mean = &#39;, mean_printed_number, &#39;, median = &#39;, median_printed_number, &#39;, and mode = &#39;, mode_printed_number, sep = &#39;&#39;)</code></pre>
<pre><code>## mean = 2.56, median = 2, and mode = 0</code></pre>
<p>Note that R doesn’t have a built in mode function. Part of the reason why is that there could be any number of modes in a data set.</p>
<p>A better way to understand our data is by plotting the relative frequency of occurrence of each printed number.</p>
<pre class="r"><code>require(scales)

ggplot() +
  geom_bar(aes(x = balls, y = ..count../sum(..count..))) +
  scale_y_continuous(labels = percent, breaks = seq(0,1,.05)) +
  scale_x_continuous(breaks = min(balls):max(balls)) +
  labs(x = &#39;printed number&#39;, y = &#39;P(number drawn = number on x-axis)&#39;) +
  theme_minimal() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())</code></pre>
<p><img src="/post/2019-02-18-a-first-attempt-at-guessing-well_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Here we can clearly see which printed numbers occur more frequently than others. Because we know the exact frequencies of each printed number and our drawing process is “random”, we can directly equate these relative frequencies with the probability of drawing a particular number. We can then refer to this plot as the probability mass function (pmf) plot. We won’t dive deep into pmfs but for now note that this plot gives us the probability of a printed number being drawn.</p>
<p>A nice property of this pmf plot is that the mode we computed numerically matches the mode we see graphically (the value with the tallest bar). We can use a different plot to visually discover our median: the cumulative distribution function (cdf) plot.</p>
<pre class="r"><code>ggplot() +
  stat_ecdf(aes(x = balls)) +
  scale_y_continuous(labels = percent, breaks = seq(0,1,.125)) +
  scale_x_continuous(breaks = min(balls):max(balls)) +
  labs(x = &#39;printed number&#39;, y = &#39;P(number drawn &lt;= number on x-axis)&#39;) +
  theme_minimal() +
  theme(panel.grid.minor.x = element_blank())</code></pre>
<p><img src="/post/2019-02-18-a-first-attempt-at-guessing-well_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>This plot tells us the probability that the printed number drawn will be less than or equal to the printed number on the x-axis. If we find the x value that corresponds with a y value of 50% we will find the median: 2.</p>
<p>We now have a lot of information about our game, but there still remains a pressing question: which number should we guess will be printed on the ball drawn? The mean, median, and mode all seem like reasonable choices (the most likely, middle value, and sum of values weighted by their probability of occurrence), but which do we choose? The answer depends on a critically important but missing feature to our game: how do we win the game?</p>
</div>
<div id="scoring-the-game-with-objective-functions" class="section level1">
<h1>Scoring the Game with Objective Functions</h1>
<p>Our game requires a winning condition. We will define a winning condition using a mathematical tool called an objective function. Objective functions are the heart and soul of optimization problems: our aim is find an input into the objective function that either maximizes or minimizes the objective function.</p>
<p>In our case we will opt for the category of objective functions called loss or cost functions, where we wish to minimize the output of the function. You can think of this as a scoring system akin to that in golf: the less points you incur during the game the better. Our loss function has to be a function of a players guess and the actual number printed on the ball that is drawn.</p>
<p>Our goal then is to find some guess that minimizes the points we accrue given our loss function. Since we don’t know what the outcome will be in advance, we will borrow from decision theory and search for the guess that minimizes our <em>expected</em> loss. We can state this in symbols as follows:</p>
<p><span class="math display">\[g^* = \underset{g}{\operatorname{arg min}} f(g)\]</span></p>
<p>Where <span class="math inline">\(g^*\)</span> is the optimal guess (the guess that minimizes the loss), <span class="math inline">\(g\)</span> is some guess, and <span class="math inline">\(f\)</span> is some loss function.</p>
<p>The question still remains: what form does our loss function actually take? There are many possibilities, but certain options give rise to new insights regarding our measures of central tendency and how they can help us in our guessing game.</p>
</div>
<div id="inequality-penalty" class="section level1">
<h1>Inequality Penalty</h1>
<p>The first loss function we will explore is perhaps the most simple. We get 1 point for guessing incorrectly and 0 points for guessing correctly (again, or goal is to get the fewest points possibility). We can refer to this as an inequality penalty.</p>
<p>In symbols:</p>
<p><span class="math display">\[d(g,x) = 1_{\{g\space\not=\space x\}}\]</span> Where <span class="math inline">\(d(g,x)\)</span> is the inequality penalty function given a guess <span class="math inline">\(g\)</span> and outcome <span class="math inline">\(x\)</span>, and <span class="math inline">\(1_{\{g\space\not=\space x\}}\)</span> is an indicator function returning <span class="math inline">\(1\)</span> if <span class="math inline">\(g\space\not=\space x\)</span> and <span class="math inline">\(0\)</span> otherwise.</p>
<p>We actually want to take an expectation over all possible outcomes, so we can write the expected penalty for a given guess <span class="math inline">\(g\)</span> as:</p>
<p><span class="math display">\[E[d(g,X)] = \sum_{i=1}^nd(g,x_i)P(x_i) = \sum_{i=1}^n1_{\{g\space\not=\space x_i\}}P(x_i)\]</span></p>
<p>As the expectation of an indicator function is the probability of the condition contained within the indicator function, we can actually reduce our expected penalty to:</p>
<p><span class="math display">\[E[d(g,X)] = P(X \not = g) = 1 - P(X = g)\]</span></p>
<p>So we now have our inequality penalty function, and we can compute its expectation for a given guess. What guess minimizes this penalty? Let’s examine it numerically and visually:</p>
<pre class="r"><code>possible_guesses &lt;- min(balls):max(balls)

expected_inequality_penalties &lt;- map_dbl(possible_guesses, function(current_guess){
  
  mean(current_guess != balls)
  
})

ggplot() +
  geom_line(aes(x = possible_guesses, y = expected_inequality_penalties)) +
  scale_y_continuous(labels = percent, breaks = seq(0,1,.05)) +
  scale_x_continuous(breaks = min(balls):max(balls)) +
  labs(x = &#39;guessed printed number&#39;, y = &#39;expected inequality penalty&#39;) +
  theme_minimal() +
  theme(panel.grid.minor.x = element_blank())</code></pre>
<p><img src="/post/2019-02-18-a-first-attempt-at-guessing-well_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The penalty is minimized at the guess of <span class="math inline">\(0\)</span>, which is the mode of our distribution. The relationship should be apparent: the penalty associated with the guess <span class="math inline">\(g\)</span> is the compliment of the probability the printed number will be <span class="math inline">\(g\)</span>. Because probabilities are bounded between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> our minimization problem has an equivalent maximization problem:</p>
<p><span class="math display">\[g^* = \underset{g}{\operatorname{arg min}} P(X \not= g) =  \underset{g}{\operatorname{arg max}} P(X = g) = Mode(X)\]</span></p>
<p>To minimize the probability that the guess is wrong we have to maximize the probability the guess is right. The printed number that has the highest probability of occurring is by definition the mode. When our scoring system only cares whether we were exactly right, we ought to choose the mode as our best guess; but what if we give ourselves some credit for guesses that are near the printed number actually drawn?</p>
</div>
<div id="absolute-difference-penalty" class="section level1">
<h1>Absolute Difference Penalty</h1>
<p>An alternative penalty function is that of the absolute difference between our guess and the actual outcome. This will gives us some credit for making guesses that are close to the actual outcome. For example, if we were to guess 5, and the outcome were 3, we would receive 2 points. If our guess was 2 and the outcome were 3, we would receive 1 point. In symbols:</p>
<p><span class="math display">\[d(g,x) = |g-x|\]</span></p>
<p>Again, we wish to take an expectation of this loss function.</p>
<p><span class="math display">\[E[d(g,X)] = E(|g-X|) = \sum_{i=1}^n |g-x_i|P(x_i)\]</span></p>
<p>What guess will minimize the expectation of our loss function? Let’s find out numerically:</p>
<pre class="r"><code>possible_guesses &lt;- min(balls):max(balls)

expected_abs_diff_penalties &lt;- map_dbl(possible_guesses, function(current_guess){
  
  mean(abs(current_guess - balls))
  
})

ggplot() +
  geom_line(aes(x = possible_guesses, y = expected_abs_diff_penalties)) +
  # scale_y_continuous(labels = percent, breaks = seq(0,1,.05)) +
  scale_x_continuous(breaks = min(balls):max(balls)) +
  labs(x = &#39;guessed printed number&#39;, y = &#39;expected absolute difference penalty&#39;) +
  theme_minimal() +
  theme(panel.grid.minor.x = element_blank())</code></pre>
<p><img src="/post/2019-02-18-a-first-attempt-at-guessing-well_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Our absolute difference penalty finds its minimum at 2, which is the median of our distribution. Why is this so? Why does the median minimize the absolute difference penalty? Let’s see if we can build an intuition.</p>
<p>Let’s begin by sorting the balls by their printed number.</p>
<pre class="r"><code>sorted_balls &lt;- sort(balls)

print(sorted_balls)</code></pre>
<pre><code>##   [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
##  [24]  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2
##  [47]  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3  3
##  [70]  3  4  4  4  4  4  4  4  4  4  4  4  4  4  4  5  5  5  5  6  6  7  7
##  [93]  7  7  7  9  9 10 10 11</code></pre>
<p>Next, let’s examine the penalty for every ball if we guess a specific number, say 3.</p>
<pre class="r"><code>abs(sorted_balls - 3)</code></pre>
<pre><code>##   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2
##  [36] 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
##  [71] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 3 3 4 4 4 4 4 6 6 7 7 8</code></pre>
<p>Rather than looking at the absolute differences that our penalty provides, let’s instead look at just the differences.</p>
<pre class="r"><code>sorted_balls - 3</code></pre>
<pre><code>##   [1] -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3
##  [24] -3 -3 -3 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -1 -1
##  [47] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0  0
##  [70]  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  3  3  4  4
##  [93]  4  4  4  6  6  7  7  8</code></pre>
<p>We can see that any outcomes less than our guess is negative, any outcome above our value is positive, and any value equal to our guess is 0. Let’s focus just on the sign of the differences.</p>
<pre class="r"><code>sign(sorted_balls - 3)</code></pre>
<pre><code>##   [1] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
##  [24] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
##  [47] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0  0
##  [70]  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
##  [93]  1  1  1  1  1  1  1  1</code></pre>
<p>We can tally the occurrence of each sign given our guess.</p>
<pre class="r"><code>table(sign(sorted_balls - 3))</code></pre>
<pre><code>## 
## -1  0  1 
## 56 14 30</code></pre>
<p>We can see that there are 56 negative values and 30 positive values given our guess of 3. What if we were guess a larger number?</p>
<pre class="r"><code>table(sign(sorted_balls - 6))</code></pre>
<pre><code>## 
## -1  0  1 
## 88  2 10</code></pre>
<p>The count of negative signs increase and the count of positive signs decrease. What if we were to guess a smaller number?</p>
<pre class="r"><code>table(sign(sorted_balls - 1))</code></pre>
<pre><code>## 
## -1  0  1 
## 26 18 56</code></pre>
<p>The count of positive signs increase and the count of negative signs decrease. Let’s look at this graphically for all possible guesses.</p>
<pre class="r"><code>map_df(possible_guesses, function(current_guess){
  
  table(sign(sorted_balls - current_guess)) %&gt;%
    tibble(sign = names(.), total = ., guess = current_guess) 
  
}) %&gt;%
  filter(sign != 0) %&gt;%
  left_join(expand.grid(sign = .$sign, guess = .$guess), ., by = c(&#39;sign&#39;,&#39;guess&#39;)) %&gt;%
  mutate(total = if_else(is.na(total), 0L, total)) %&gt;%
  ggplot() +
  geom_col(aes(x = guess, y = total, fill = sign), position = &#39;dodge&#39;) +
  scale_x_continuous(breaks = min(balls):max(balls)) +
  labs(x = &#39;guessed printed number&#39;, y = &#39;count of observations&#39;) +
  theme_minimal() +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank())</code></pre>
<p><img src="/post/2019-02-18-a-first-attempt-at-guessing-well_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>We see here that as our guess moves to the right of the median, the count of negative signs increases. The opposite is true as our guess moves to the left of the median. When we guess the median, the number of negative and positive signs is equal. As a guess, the median perfectly balances the number of negative and positive differences.</p>
</div>
<div id="squared-difference-penalty" class="section level1">
<h1>Squared Difference Penalty</h1>
<p>We have seen how the mode and the median are optimal given an inequality and absolute difference penalty respectively. What role does the mean play? The answer arises when we wish to penalize larger errors worse than smaller errors.</p>
<p>Let’s imagine that we want to give ourselves more credit when we are very near to the right answer, but penalize ourselves when we are very far. One way to do this would be to take the squared differences between our guess and the actual outcome. For example, if we guess <span class="math inline">\(2\)</span>, and the result is <span class="math inline">\(3\)</span>, we get <span class="math inline">\(1^2=1\)</span> point; if we guess <span class="math inline">\(2\)</span> and the result is <span class="math inline">\(4\)</span> we get <span class="math inline">\(2^2=4\)</span> points, and if we guess <span class="math inline">\(2\)</span> and the result is <span class="math inline">\(10\)</span> we get <span class="math inline">\(8^2 = 64\)</span> points. It becomes much more more important to avoid large errors.</p>
<p>Here is our cost function in symbols:</p>
<p><span class="math display">\[d(g,x) = (g-x)^2\]</span></p>
<p>As per usual, we would like to take this as an expectation.</p>
<p><span class="math display">\[E[d(g,X)] = E[(g-X)^2] = \sum_{i=1}^n (g-x_i)^2P(x_i)\]</span></p>
<p>Let’s find the minimum of this cost function numerically and visually:</p>
<pre class="r"><code>possible_guesses &lt;- seq(min(balls), max(balls),.01)

expected_sqr_diff_penalties &lt;- map_dbl(possible_guesses, function(current_guess){
  
  mean((current_guess - balls)^2)
  
})

ggplot() +
  geom_line(aes(x = possible_guesses, y = expected_sqr_diff_penalties)) +
  # scale_y_continuous(labels = percent, breaks = seq(0,1,.05)) +
  scale_x_continuous(breaks = min(balls):max(balls)) +
  labs(x = &#39;guessed printed number&#39;, y = &#39;expected squared difference penalty&#39;) +
  theme_minimal() +
  theme(panel.grid.minor.x = element_blank())</code></pre>
<p><img src="/post/2019-02-18-a-first-attempt-at-guessing-well_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>The expected squared difference penalty bottoms out at 2.56, which is the mean of our distribution. An analytic derivation of the mean as a solution to squared distances can be found <a href="https://math.stackexchange.com/questions/2554243/understanding-the-mean-minimizes-the-mean-squared-error">on stack exchange</a>. We won’t cover the details here.</p>
<p>So the mean is the best guess when were are being penalized by the squared difference between our guess and the actual outcome. We now can associate each of the three most common measures of central tendency as being the best guess given some scoring system that penalizes us for guessing incorrectly. In this way we see measures of central tendency as solutions to variational problems, a concept taken from the calculus of variations.</p>
</div>
<div id="lp-metrics" class="section level1">
<h1>Lp metrics</h1>
<p>Before we close,we should touch on the common thread that spans all three of the cost functions we discussed: the concept of Lp metrics.</p>
<p>All of our cost functions discussed so far are special cases of a more general form: <span class="math display">\[d(g,x) = |g-x|^p\]</span></p>
<p>We will refer to this general form as an Lp metric. We note here that Lp metrics are actually far more general than what we will cover in this post, and will be a recurring topic as we dive deeper into machine learning in particular. For now though, we will continue to discuss the form we have just described.</p>
<p>Each of our cost functions is a special case of the general form. When <span class="math inline">\(p = 1\)</span>, we get the absolute difference penalty. When <span class="math inline">\(p = 2\)</span>, we get the squared difference penalty. We define the limit $p 0 $ as the inequality penalty. In this way, as <span class="math inline">\(p\)</span> gets smaller we begin to treat any difference as being equally bad, whereas when <span class="math inline">\(p\)</span> gets larger bigger differences matter much much more. What happens when <span class="math inline">\(p \to \infty\)</span>?</p>
<p>Let’s demonstrate what happens in R:</p>
<pre class="r"><code>map_df(c(2^c(0:7)), function(p){
  
 tibble(x = seq(0,11,.01), y = x^p, p = p)
  
}) %&gt;%
  group_by(p) %&gt;%
  mutate(y = y/sum(y)) %&gt;%
  ggplot() +
  geom_line(aes(x = x, y = y, color = factor(p))) +
  theme_minimal() +
  scale_color_discrete(name = &#39;p&#39;) +
  labs(x = &#39;difference&#39;, y = &#39;normalized difference^p&#39;)</code></pre>
<p><img src="/post/2019-02-18-a-first-attempt-at-guessing-well_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>On the x axis we have the absolute value of the difference between a guess and an actual outcome. On the y axis we have difference raised to a power <span class="math inline">\(p\)</span>. The y axis has been normalized so that we can see all of the lines together. From here the trend as <span class="math inline">\(p \to \infty\)</span> is quite apparent: only the largest differences matter. What this results in is a penalty function that gives as many points as the maximum difference:</p>
<p><span class="math display">\[E[d(g,X)] = max|g-X|\]</span></p>
<p>What value minimizes this extreme cost function?</p>
<pre class="r"><code>possible_guesses &lt;- seq(min(balls),max(balls),.01)

expected_max_diff_penalties &lt;- map_dbl(possible_guesses, function(current_guess){
  
  max(abs(current_guess - balls))
  
})

ggplot() +
  geom_line(aes(x = possible_guesses, y = expected_max_diff_penalties)) +
  # scale_y_continuous(labels = percent, breaks = seq(0,1,.05)) +
  scale_x_continuous(breaks = min(balls):max(balls)) +
  labs(x = &#39;guessed printed number&#39;, y = &#39;expected maximum difference penalty&#39;) +
  theme_minimal() +
  theme(panel.grid.minor.x = element_blank())</code></pre>
<p><img src="/post/2019-02-18-a-first-attempt-at-guessing-well_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The expected maximum difference penalty bottoms out when we guess the number that is halfway between our minimum and maximum value i.e. the midrange. When we want to minimize the single largest difference we ought to choose the midrange. This is an extreme cost function, and one that has a severe limitation: the maximum and the minimum of the possible outcomes must be defined. While this works fine for our current guessing game, it is impossible to compute when talking about a probability distribution missing an upper or lower bound.</p>
</div>
<div id="aftermath" class="section level1">
<h1>Aftermath</h1>
<p>We covered a lot of ground in this post, but here is a brief recap:</p>
<ul>
<li><p>The mean, median, and mode are all measures of central tendency: numbers that represent typical values in a data set.</p></li>
<li><p>When trying to make a good guess using a measure of central tendency, we have to know how we are defining success i.e. what is the penalty if we make a mistake?</p></li>
<li><p>Use the mode when minimizing the penalty for inequality; use the median when minimizing the absolute difference penalty; use the mean when minimizing the squared difference penalty; and use the midrange (if it exists) when minimizing the maximum difference penalty.</p></li>
</ul>
<p>Often we will come across problems when cost functions are ill defined (if at all), and we don’t have complete knowledge of the outcomes or their probabilities. This makes the problem of guessing well replete with challenges that we will need to build an arsenal of tools to solve.</p>
<p>Until next time,</p>
<p>W.C.</p>
</div>

  </div>

  <div id=links>
    
    
      <a class="basic-alignment left" href="/post/some_oddities_of_the-principle-of-insufficient-reason/">Some Oddities of the Principle of Insufficient Reason &raquo;</a>
    
  </div>
</section>

<section id="comments">
<div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
      
      
      if (window.location.hostname == "localhost")
                return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = '';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
  
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>



</body>
</html>

