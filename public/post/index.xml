<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on On Guessing Well</title>
    <link>/post/</link>
    <description>Recent content in Posts on On Guessing Well</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 02 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The Prediction Problem</title>
      <link>/post/the-prediction-problem/</link>
      <pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-prediction-problem/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bernoulli and Binomial Distribution</title>
      <link>/post/bernoulli-and-binomial-distribution/</link>
      <pubDate>Mon, 19 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bernoulli-and-binomial-distribution/</guid>
      <description>Today’s post will explore a fundamental probability distribution and its generalization: the Bernoulli and binomial distribution. The Bernoulli distribution is named after Jacob Bernoulli, one of several famous mathematicians from the Bernoulli family. We can use this distribution to represent the probability of a single experiment with a binary (bīnārius is latin for “consisting of two”) outcome. Classically, this probability setup can be thought of as the outcome of a single coin toss, however it can be generalized to a great many practical applications.</description>
    </item>
    
    <item>
      <title>A Closer Look at Inductive Generalization and the Statistical Syllogism</title>
      <link>/post/a-closer-look-at-inductive-generalization-and-the-statistical-syllogism/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/a-closer-look-at-inductive-generalization-and-the-statistical-syllogism/</guid>
      <description>A recent post compared and contrasted deduction, induction, and abduction. Within that post we touched on inductive generalization (inferring characteristics of population from a sample), and the statistical syllogism (inferring characteristics of a sample from a population). I briefly wanted to spend a little more time with each of these concepts, and talk about what conclusions are plausible (i.e. not necessarily true or false given what we know) when applying them.</description>
    </item>
    
    <item>
      <title>A Closer Look at the Statistical Syllogism and Inductive Generalization</title>
      <link>/post/a-closer-look-at-the-statistical-syllogism-and-inductive-generalization/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/a-closer-look-at-the-statistical-syllogism-and-inductive-generalization/</guid>
      <description>A recent post compared and contrasted deduction, induction, and abduction. Within that post we touched on both the statistical syllogism (inferring characteristics of a sample from a population), and inductive generalization (inferring characteristics of population from a sample). I wanted to spend a little more time with each of these concepts to get a better sense of their justification and how we can interpret them. My hope is that through this post, we will gain some intuition of probable reasoning; and experience the interplay between induction, abduction, and deduction.</description>
    </item>
    
    <item>
      <title>Chinese Resaurants and Indian Buffets</title>
      <link>/post/chinese-restaurants-and-indian-buffets/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/chinese-restaurants-and-indian-buffets/</guid>
      <description>Today I wanted to talk about one of my all time favorite subjects in probability theory: stochastic processes. Many stochastic processes can be thought of as sequences of random variables. For instance a Bernoulli process is a sequence of binary random variables, which can be though of as a listing of the outcomes of successive coin flips. Today we are going to talk about an interesting set of stochastic processes that are closely related to one of the most fundamental idea in probability and statistics: sampling.</description>
    </item>
    
    <item>
      <title>The Principle of Insufficient Reason</title>
      <link>/post/the-principle-of-insufficient-reason/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-principle-of-insufficient-reason/</guid>
      <description>  Definition History Example Rule of Succession Apparent Paradox  Coin with three sides Continious Distributions  Generalization via the Principle of Maximum Entropy  </description>
    </item>
    
    <item>
      <title>The Principle of Maximum Entropy</title>
      <link>/post/the-principle-of-maximum-entropy/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-principle-of-maximum-entropy/</guid>
      <description>  Information Content/Surprisal Entropy Maximizing Entropy Given Constraints Lagrangian Multipliers Exponential Distributions Applications Likelihoods Priors Example  </description>
    </item>
    
    <item>
      <title>An Introduction to Deductive Reasoning</title>
      <link>/post/an-introduction-to-deductive-reasoning/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/an-introduction-to-deductive-reasoning/</guid>
      <description>One of my absolute favorites books is Probability: the Logic of Science by E.T. Jaynes. The book was published posthumously in 2003, after Jayne’s death in 1998. While the book presents a compelling philosophical argument for probability theory, it is known to be incomplete and to contain errors. Despite this, I think it is a very important work in the history of probability theory, and a great foundation for how one can reason with incomplete information.</description>
    </item>
    
    <item>
      <title>Reasoning via Deduction, Induction, and Abduction</title>
      <link>/post/reasoning-via-deduction-induction-and-abduction/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/reasoning-via-deduction-induction-and-abduction/</guid>
      <description>My last post was an introduction to deductive reasoning, that is reasoning via deductive argument. I wanted to briefly outline two other forms of reasoning so we can identify their usage and compare their strengths and weakness in future applications.
Deduction As we stated in our last post, deduction is reasoning that, given premises, guarantees the truth of its conclusion. Two major forms of deductive argument are “Modus Ponens” and “Modus Tollens”.</description>
    </item>
    
    <item>
      <title>Probability in a Finite and Deterministic Universe</title>
      <link>/post/probability-in-a-finite-and-deterministic-universe/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/probability-in-a-finite-and-deterministic-universe/</guid>
      <description>My previous post got me thinking about the nature of infinity. It lead me to a thought provoking post by Steve Patterson on logical errors regarding infinite sets. While I haven’t taken enough time to fully appreciate his logic, it got me thinking about how probability relates to metaphysics and epistemology. Steve’s claim is that modern mathematics has made a mistake with regards to infinity, specifically that by definition a set cannot be infinite (or else it would be finite), and thus arguments built on this premise are unsound (though the logic, granting the premise may be valid).</description>
    </item>
    
    <item>
      <title>Some Oddities of the Principle of Insufficient Reason</title>
      <link>/post/some_oddities_of_the-principle-of-insufficient-reason/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/some_oddities_of_the-principle-of-insufficient-reason/</guid>
      <description>We are keenly interested in reasoning with incomplete or uncertain information i.e. how we should make a “best guess” given that we have imperfect knowledge. One of the classical tools in our tool belt is the so called “principle of insufficient reason” also known as the “principle of indifference”. More on the history and extensions of the principle, but for now I want to focus on some oddities that occur by trying to reason with the principle.</description>
    </item>
    
    <item>
      <title>An Introduction to Workload Modeling</title>
      <link>/post/an-introduction-to-workload-modeling/</link>
      <pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/an-introduction-to-workload-modeling/</guid>
      <description>The Purpose of this Post This post presents a brief introduction to workload modeling. For our purposes we define workload modeling as “an effort to usefully represent the relationship between A) the demand for products (or services), B) the production (or service delivery) process, C) and the requirement for resources”. This should be taken as a working definition, as we are sure that our own investigation into this subject will shape and change our fundamental understanding in the future.</description>
    </item>
    
    <item>
      <title>A First Attempt w/ Central Tendency</title>
      <link>/post/a-first_attempt-with-central-tendency/</link>
      <pubDate>Mon, 18 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/a-first_attempt-with-central-tendency/</guid>
      <description>The purpose of this blog is to explore how we can reason about what we know to make guesses about what we don’t. We are going to “dip our toes” into this subject by exploring one of the most commonly experienced concepts in probability theory and statistics: measures of central tendency. Specifically, we are going to revisit the common definitions of measures such as the mean, median, and mode; and attempt to build a stronger intuition of what they are and why they are useful in our goal of making better guesses.</description>
    </item>
    
  </channel>
</rss>