---
title: 'Reasoning via Deduction, Induction, and Abduction'
author: David W.C. Telson
date: '2019-07-14'
slug: reasoning-via-deduction-induction-and-abduction
categories: []
tags: []
draft: FALSE
---

My last post was an introduction to deductive reasoning, that is reasoning via deductive argument. I wanted to briefly outline two other forms of reasoning so we can compare and contrast their merits, as well as identify their usage.

# Deduction

As we stated in our last post, deduction is reasoning that, given premises, guarantees the truth of its conclusion. Two major forms of deduction are "Modus Ponens" and "Modus Tollens". For two arbitrary propositions $P$ and $Q$, these rules of inference take the following forms:

**Modus Ponens i.e. Affirm by Affirming:**

$$
\begin{aligned}
& \ P \implies Q & \text{If P is true, Q is also true.}\\ 
& \ P & \text{P is true.}\\ 
\hline
& \therefore \ Q & \text{Therefore Q is also true.}\\ 
\end{aligned}
$$

**Modus Tollens i.e. Deny by Denying:**

$$
\begin{aligned}
& \ P \implies Q & \text{If P is true, Q is also true.}\\ 
& \ \neg Q & \text{Q is false.}\\ 
\hline
& \therefore \ \neg P & \text{Therefore P is also false.}\\ 
\end{aligned}
$$

The truth of these conclusions is necessary given the premises (here "given" means assuming the premises are true). This can be seen via a truth table for the implication operator $\implies$:

$$
\begin{array}{|c|c|c|}
P & Q & P \implies Q\\ 
\hline 
T & T & T\\
T & F & F\\
F & T & T\\
F & F & T\\
\end{array}
$$

In this table there is only one row where $P$ and $P \implies Q$ are true, and in this row $Q$ is also true. Analogously there is only one row where $P \implies Q$ is true and $Q$ is false, which also has a false value for $P$. This is what necessitates the truth of the conclusions given the premises. Note that this doesn't guarantee the truth of the premises: we assumed they were true.

# Induction

Induction can be defined as reasoning from a specific premise to a general conclusion. The English philosopher David Hume (of whom we will have much more to say) referred to induction as reasoning that "instances of which we have had no experience resemble those of which we have had experience." 

In some cases induction can be seen as the opposite of deduction in that deduction begins with a general premise and reasons about specific conclusions. A more informative way to differentiate the two is thus: the premises of a deductive argument makes the truth value of a conclusion absolutely certain; whereas the premises of an inductive argument only lend evidence to a conclusion probabilistically.

Using induction we could make a variety of arguments to include the following examples:

**Generalization**

If we observe a sample from a larger population, and some percent of the sample possesses a quality, we can infer that it is likely that some percent of the population also possesses the quality. Note that some could be 100% i.e the entire sample. Also note that the size and representativeness of the sample has an influence on the strength of our assertion.

$$
\begin{aligned}
& Q(x)\\
& x \in \mathcal{X}\\ 
\hline
& \therefore Q(\mathcal{X})\\\\
\end{aligned}
$$

**Statistical Syllogism**

If we know that some percent of members of a population possess a quality, then we can infer that that there is some percent chance that a member possesses the quality. Note that how we select a member from the population is important (e.g. random draw), as a sub-population may have a different proportion of members with the certain quality. We could also pair generalization with a statistical syllogism: inferring a characteristic of a member of a population from the characteristics of a sample from a population.

$$
\begin{aligned}
& Q(\mathcal{X})\\
& x \in \mathcal{X}\\ 
\hline
& \therefore Q(x)\\\\
\end{aligned}
$$

**Inductive Analogy**

If we know that two objects share a certain set of characteristics, and that the first object also possesses an additional characteristic, then it is likely that second shares this characteristic as well. Note that the number of known shared characteristics (as well as the number of known unshared characteristics) influences the strength of our argument. This kind of reasoning can be seen as equivalent to the composition of generalization and statistical syllogism. 

$$
\begin{aligned}
& Q(x) \wedge Q(y)\\
& R(x)\\ 
\hline
& \therefore R(y)\\\\
\end{aligned}
$$

**Predicting the Future Based Upon the Past**

If we know that object has always possessed a certain quality before a certain time, then we can infer that it will possess the quality on and after time. This is very akin to generalization where all of time is the population and the past is the sample. Arguably the biggest weakness to this form of induction is that the past history of an object is likely both A) not very large relative to its potential future, and B) not very representative relative to its potential future.

$$
\begin{aligned}
& \forall t < t_0 : Q(x|t)\\
& \forall t < t_0: t \in \mathcal{T} =\{t : t < t_0\} \cap  \{t : t \ge t_0\}\\ 
\hline
& \therefore \forall t \in \mathcal{T} : Q(x|t)\\\\
\end{aligned}
$$

### Justifying Induction

As we previously mentioned, induction does not guarantee its conclusions. Then how can we justify using this kind of reasoning? One might propose that we can justify induction because in the course of human history, induction has worked out very well for us. A keen observer will note that this justification is equivalent to the form of induction described by "predicting the future based upon the past". This would be justifying induction by induction! 

David Hume explored and popularized this "Problem of Induction". Essentially Hume showed that it was impossible to justify induction deductively, and justification via induction was unacceptable (as it is circular). The philosopher Nelson Goodman expanded the woes of induction with his "New Riddle of Induction" which exacerbates the issue by introducing concepts like "Grue" and "Bleen" to illustrate the fact that induction stands on even shakier footing than we initial thought.

Many philosophers (e.g. Emmanuel Kant) attempted to find satisfactory resolution to the problem of induction to no avail. In the mid 20th century, a German philosopher of science named Karl Popper proposed a solution: accept the problem of induction renders induction unjustifiable, and instead frame the growth of scientific knowledge as a program of learning from error via Modus Tollens i.e. scientists learn by subjecting their theory to falsification, rather than confirmation. Falsificationism will be of great use to us in the future.

# Abduction

Abduction can be described as "reasoning to the most likely explanation." For example, supposing we were awaken one night to a sound coming from upstairs (say from the attic of a single story home). There are a number of possible sources for the noise, such as a couple of boxes falling over, a bat having found its way into your home, or perhaps a gremlin has snuck into your home and is causing havoc. Abduction would have you reason about how likely each conclusion is, and choose the one with the highest likelihood.  

Abduction is well described by the famous Sherlock Holmes (really Sir Arthur Conan Doyle, I suppose) quote: "once you've eliminated the impossible, whatever remains, no matter how improbable, must be the truth." Essentially this describes a process of collecting evidence and applying a composition of Modus Tollens and abduction. Maximum Likelihood Estimation (MLE) in frequentist statistics, and Bayesian inference as a whole could be considered as forms of abduction, almost by definition. 

It should also be noted that formally speaking, abduction takes a form similiar (if not identical) the logical fallacy "Affirming the Consequent," albeit with a weaker conclusion. In abduction we would have observed some event $Q$, and we would have a set of possible explainations of $Q$, $P = \{P_1, P_2, ... P_n\}$. 

For each $P_i \in P$, we have:
$$
\begin{aligned}
& \ P_i \implies Q \\ 
& \ Q \\ 
\hline
& \therefore \ P_i \ \text{is plausible}\\ 
\end{aligned}
$$

Abduction as a rule requires one additional premise to be added to the argument. Imagine we are able to measure the plausibility, or how well an explaination fits the observation, with a function $R$. The higher the value of $R$, the better the explaination fits the observation.

$$
\begin{aligned}
& \ Q \\
& \ \forall P_i \in P : P_i\implies Q \\ 
\hline
& \therefore \ P_i^* := argmax_i\  R(P_i)\\ 
\end{aligned}
$$

Like induction, abduction does not garauntee its conclusions. It is frought with similiar difficulties in justification, but also comes with unique challenges in rationalizing its use. Despite this, we will find abduction as a very useful tool as we explore how to reason about uncertain or incomplete information.

Another note on how abduction relates to induction is that often in determining the likelihood of possibilities, we often use induction! For example, I have never seen a Gremlin, therefore I don't think it is very likely that a Gremlin is the cause of my mystery noise, even though a Gremlin perfectly explains the noise. Conversely, I have a lot of boxes in my attic and boxes have fallen in my attic in the past. Using this line of reasoning, I can say that the conclusion that a box fell in the attic is the best explaination of the noise both in terms of my prior experience with the possibilities, and how well the possibilities explain the noise. 













