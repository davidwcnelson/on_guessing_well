---
title: "Probability in a Finite and Deterministic Universe"
author: "David W.C. Telson"
date: '2019-07-13'
slug: probability-in-a-finite-and-deterministic-universe
tags: []
categories: []
---



<p>My previous post got me thinking about the nature of infinity. It lead me to a thought provoking post by Steve Patterson on <a href="http://steve-patterson.com/cantor-wrong-no-infinite-sets/">logical errors regarding infinite sets</a>. While I haven’t taken enough time to fully appreciate his logic, it got me thinking about how probability relates to metaphysics and epistemology. Steve’s claim is that modern mathematics has made a mistake with regards to infinity, specifically that by definition a set cannot be infinite (or else it would be finite), and thus arguments built on this premise are invalid (though the logic, granting the premise may be sound). Steve does a great job illustrating this point in numerous posts, which I won’t dive into here. The least I can do is encourage you to check them out for yourself.</p>
<p>All of this reminded me of a John von Neuman quote, which I paraphrase as “tell me precisely what a machine can’t do, and I will build a machine that can do just that”. My interpretation of what von Neuman means is that a machine is only limited by our ability to provide a precise definition. Computers can’t represent infinite sets, or at least they can’t truly represent infinite sets (just bounded finite rational approximations). Steve Patterson argues this is because infinite sets don’t exist, as this contradicts their conception. Of course, computers can represent an arbitrarily large collection of objects (or an arbitrarily precise rational approximation of a real number), limited only by the finite memory of the computer and the time it takes to compute it.</p>
<p>What’s another concept that a computer can’t truly represent? How about this: randomness. A computer can’t produce a truly random number (only a pseudo-random one). But what is a truly random number? What is meant by “randomness”? Some would describe a random outcome as one that has no discernible rule that determined it. Is this even possible? Its hard to conceive, but that doesn’t necessarily make it impossible. Randomness doesn’t seem to be a contraction in the same way that an infinite set is. Is our definition of randomness sufficient to describe what we mean? What about all of that talk in quantum physics about the universe being inherently random? This brings back the premises of the Bohr and Einstein debates, <a href="https://bayes.wustl.edu/etj/articles/cmystery.pdf">my favorite response to which is from E.T. Jaynes</a>. All of the philosophical debates aside, what would probability theory look like in a universe that is both finite and non-random i.e. deterministic?</p>
<p>Let’s briefly explore this concept.</p>
<p>Suppose that there is an some outcome of a process that interests us. We will refer to it as <span class="math inline">\(y\)</span> and to us the value of <span class="math inline">\(y\)</span> is unknown, but since the universe is finite, the number of possible values <span class="math inline">\(y\)</span> could be is also finite. Let’s presume we can enumerate the possible outcomes and collect them in the set <span class="math inline">\(Y = \{y_1, y_2,...,y_n\}\)</span>. Because the world is deterministic and we have enumerated all the possible outcomes via <span class="math inline">\(Y\)</span>, we have that <span class="math inline">\(y\)</span> must be equal to one and only one element of <span class="math inline">\(Y\)</span>.</p>
<p>Since the universe is finite and deterministic, there exists a finite ordered tuple of observed variables <span class="math inline">\(x = (x^{(1)},x^{(2)},...,x^{(m)}\}\)</span> whose values completely determine the value of <span class="math inline">\(y\)</span>. In other words, <span class="math inline">\(y = f(x_0)\)</span>. There are no ifs, ands, or buts about it. <span class="math inline">\(y\)</span> is completely determined by <span class="math inline">\(x_0\)</span>. Thus the probability that <span class="math inline">\(y = y_i\)</span> for some <span class="math inline">\(y_i\)</span> is either <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span>, being <span class="math inline">\(1\)</span> for exactly one <span class="math inline">\(y_i \in Y\)</span>.</p>
<p>But what if we were missing the value of one of the determining variables e.g. <span class="math inline">\(x^{(j)}\)</span>? This is where probabilistic reasoning has its first non-trivial introduction into our finite and deterministic world. Since our world is finite, <span class="math inline">\(x^{(j)}\)</span> must only be able to take on some finite number of possible values. In our case, let’s capture these in the set <span class="math inline">\(X^{(j)} = \{x^{(j)}_1, x^{(j)}_2,..., x^{(j)}_k\}\)</span>. Let’s refer to the incomplete set of observed variables (missing only <span class="math inline">\(x^{(j)}\)</span>) as <span class="math inline">\(x^{(-j)}\)</span>. Since we know the possible values of <span class="math inline">\(x^{(j)}\)</span>, we could compute <span class="math inline">\(f(x^{(-j)},x_h^{(j)})\)</span> for each <span class="math inline">\(x_h^{(j)} \in X^{(j)}\)</span>. For each computation we would end up with a value from <span class="math inline">\(Y\)</span> i.e. <span class="math inline">\(f(x^{(-j)},x_h^{(j)}) \in Y\)</span>. Some <span class="math inline">\(y_i\)</span> will be featured more than once, others might not be featured at all. Regardless we will have a set that yields a <span class="math inline">\(y_i \in Y\)</span> for each <span class="math inline">\(x_h^{(j)}\)</span> given <span class="math inline">\(x^{(-j)}\)</span> and <span class="math inline">\(f\)</span> (this “given” part will come up in a bit).</p>
<p>After performing the finite computation, we will can compute probabilities as follows:</p>
<p><span class="math display">\[P(y = y_i|x^{(-j)}) = \frac{1}{k}\sum_{h=1}^k 1_{\{y_i = f(x^{(-j)},x_h^{(j)})\}}\]</span></p>
<p>That is, the probability that <span class="math inline">\(y = y_i\)</span> given <span class="math inline">\(x^{(-j)}\)</span> is exactly the number of times that <span class="math inline">\(f(x^{(-j)},x_h^{(j)})\)</span> would produce <span class="math inline">\(y_i\)</span> when considering all possible values of <span class="math inline">\(x^{(j)}\)</span>. This is a rather interesting result. In a finite and deterministic world, given a subset of the finite set of variables that determine <span class="math inline">\(y\)</span>, we can speak of the probability that <span class="math inline">\(y\)</span> takes on a specific value <span class="math inline">\(y_i\)</span> as the number of times <span class="math inline">\(y_i\)</span> is computed considering all combinations of the possible values of the missing variables. This seems like an incredibly straight forward and reasonable approach to probability, but hold on, there are a number of items we have not addressed:</p>
<ol style="list-style-type: decimal">
<li><p>What determines the values of <span class="math inline">\(x\)</span>? Is there some finite regress that gets us back to an initial condition? It can’t be infinite, as we said the universe is finite. It can’t be random, as we said the universe is deterministic.</p></li>
<li><p>Are some values of <span class="math inline">\(x^{(j)}\)</span> in some way more likely than others? What determines this? Somehow this is related to our first point. If likelihoods are different, should we not consider these likelihoods in computing the probability of <span class="math inline">\(y\)</span>?</p></li>
<li><p>How do we know what the elements of <span class="math inline">\(x\)</span> are? Is there some sense that we can tell if <span class="math inline">\(x\)</span> is missing variables? How do we deal with redundant variables i.e. variables that are excessive or not necessary. My thought is that this would relate to the variation in observations of <span class="math inline">\(y\)</span> over time.</p></li>
<li><p>Where does <span class="math inline">\(f\)</span> come from? Is it an element of <span class="math inline">\(x\)</span>? If its form is dependent on the members of <span class="math inline">\(x\)</span>, and it is required to determine <span class="math inline">\(y\)</span>, then certainly it must be in <span class="math inline">\(x\)</span>. If we don’t know <span class="math inline">\(f\)</span>, can we consider a set of possible functions?</p></li>
<li><p>What do we do if we don’t know any elements of <span class="math inline">\(x\)</span> i.e. we are completely ignorant of the variables and function that determine <span class="math inline">\(y\)</span>? My sense is that this is where the principle of insufficient reason enters the scene.</p></li>
<li><p>How do we start from a state of ignorance about <span class="math inline">\(x\)</span> to a state of certainty about <span class="math inline">\(x\)</span>? Our certainty about <span class="math inline">\(x\)</span> directly relates to our certainty about <span class="math inline">\(y\)</span>. My intuition is that this is where collecting data and making observations come from.</p></li>
</ol>
<p>There are many more questions we could ask, but for now I am going to leave this be. While it might be too strong to assume the universe is finite and deterministic, we can at least say that a computable universe must be i.e. if we wish to have a probability theory that is computable, it will have to represent the world in a finite, deterministic way.</p>
